{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting prompts from SWE-Bench\n",
    "\n",
    "This notebook extracts prompts from the SWE-bench dataset to be used by humans and language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'squad' dataset\n",
    "dataset = load_dataset(\"princeton-nlp/SWE-bench_oracle_llama\", split='test')\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available repos\n",
    "df[\"repo\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_rnd_issue(df: pd.DataFrame, repo: str=None, dir: str=\"./prompts_oracle/\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Fetches a random issue from the given DataFrame and saves its prompt to a file in `dir`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    # If repo is specified, filter rows by the given repo name.\n",
    "    if repo:\n",
    "        df = df[df['repo'] == repo]\n",
    "    \n",
    "    # Return a random row from the filtered DataFrame.\n",
    "    issue =  df.sample(n=1).iloc[0]\n",
    "\n",
    "    text = issue['text']\n",
    "    instance_id = issue['instance_id']\n",
    "\n",
    "    # Create a filename using the \"instance_id\"\n",
    "    filename = f\"{dir}/{instance_id}.txt\"\n",
    "\n",
    "    # Save the \"text\" to the file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"Save the issue prompt to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rnd_issue(df, repo=\"django/django\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of patch\n",
    " \n",
    "Enter your patch and instance_id (usually the file name of prompt with `.txt`) below and run the cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_id=\"pallets__flask-4642\"\n",
    "patch_text=\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"tmp_predictions.json\"\n",
    "json_dict = {\n",
    "    \"instance_id\": instance_id,\n",
    "    \"prediction\": patch_text,\n",
    "    \"model\":\"human\"\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as json_file:\n",
    "    # the eval script expects predictions to be in a list\n",
    "    json.dump([json_dict], json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def run_evaluation(log_dir=\"evaluation_outputs\", swe_bench_tasks=\"../swe-bench.json\", testbed=\"eval-artifacts-deleteme\", skip_existing=None, timeout=None, verbose=None):\n",
    "    cmd = [\n",
    "        \"python\", \"../harness/run_evaluation.py\",\n",
    "        \"--predictions_path\", \"tmp_predictions.json\",\n",
    "        \"--log_dir\", log_dir,\n",
    "        \"--swe_bench_tasks\", swe_bench_tasks,\n",
    "        \"--testbed\", testbed\n",
    "    ]\n",
    "\n",
    "    for dir in [log_dir,testbed]:\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "\n",
    "    # Add optional arguments to the command\n",
    "    if skip_existing is not None:\n",
    "        cmd.extend([\"--skip_existing\", str(skip_existing)])\n",
    "    \n",
    "    if timeout is not None:\n",
    "        cmd.extend([\"--timeout\", str(timeout)])\n",
    "\n",
    "    if verbose is not None:\n",
    "        cmd.extend([\"--verbose\", str(verbose)])\n",
    "\n",
    "    # Run the command\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "    # Print the output\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "\n",
    "    # Return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
